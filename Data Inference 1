
import requests
from bs4 import BeautifulSoup
import json
import re
import logging
from typing import List, Dict, Optional
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import pipeline

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='globeguard_inference.log'
)
logger = logging.getLogger(__name__)

# Configuration for risk classification
RISK_KEYWORDS = {
    'high': ['outbreak', 'spreading rapidly', 'critical', 'severe', 'epidemic'],
    'moderate': ['increasing', 'cases rising', 'potential risk', 'developing'],
    'low': ['mild', 'under control', 'isolated']
}

class GlobalHealthDataInference:
    def __init__(self, sources=None):
        """
        Initialize data inference with optional custom sources.
        
        Args:
            sources (List[str], optional): List of URLs to scrape
        """
        self.sources = sources or [
            "https://www.cdc.gov/travel", 
            "https://www.who.int/emergencies"
        ]
        self.classifier = pipeline(
            "text-classification", 
            model="distilbert-base-uncased",
            top_k=None
        )
    
    def scrape_global_data(self, url: str) -> List[Dict[str, str]]:
        """
        Scrape data from a given URL with robust error handling.
        
        Args:
            url (str): URL to scrape
        
        Returns:
            List of dictionaries containing scraped data
        """
        try:
            response = requests.get(
                url, 
                timeout=10, 
                headers={'User-Agent': 'GlobeGuard Health Tracker'}
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            articles = soup.find_all(["article", "div"], class_=re.compile("article|news"))
            
            data = []
            for art in articles:
                title = art.find(["h2", "h1"])
                summary = art.find(["p", "div"], class_=re.compile("summary|content"))
                
                if title and summary:
                    data.append({
                        "title": title.get_text(strip=True),
                        "summary": summary.get_text(strip=True)
                    })
            
            return data
        except requests.RequestException as e:
            logger.error(f"Scraping error for {url}: {e}")
            return []
    
    def preprocess_text(self, text: str) -> str:
        """
        Clean and normalize text for analysis.
        
        Args:
            text (str): Input text
        
        Returns:
            Cleaned text
        """
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s]', '', text)
        return text.lower().strip()
    
    def classify_risk(self, content_list: List[str]) -> List[Dict[str, str]]:
        """
        Classify risk levels using multiple strategies.
        
        Args:
            content_list (List[str]): List of text contents
        
        Returns:
            List of dictionaries with content and risk classification
        """
        classifications = []
        
        for content in content_list:
            # Keyword-based initial classification
            risk_level = 'low'
            for level, keywords in RISK_KEYWORDS.items():
                if any(keyword in content.lower() for keyword in keywords):
                    risk_level = level
                    break
            
            # NLP model refinement (optional, weighted)
            try:
                nlp_result = self.classifier(content)
                for result in nlp_result[0]:
                    if result['score'] > 0.7:
                        # Potential model-based refinement logic
                        pass
            except Exception as e:
                logger.warning(f"NLP classification error: {e}")
            
            classifications.append({
                "content": content,
                "risk": risk_level.capitalize()
            })
        
        return classifications
    
    def process_health_data(self, mock_local_data: Optional[List[Dict]] = None) -> List[Dict]:
        """
        Comprehensive data processing pipeline.
        
        Args:
            mock_local_data (Optional[List[Dict]]): Optional local data for testing
        
        Returns:
            Processed and classified health data
        """
        global_data = []
        for source in self.sources:
            global_data.extend(self.scrape_global_data(source))
        
        # Include mock data if provided
        if mock_local_data:
            global_data.extend([
                {
                    "title": item.get("headline", ""),
                    "summary": item.get("content", "")
                } for item in mock_local_data
            ])
        
        processed_data = [
            self.preprocess_text(item.get('summary', '')) 
            for item in global_data
        ]
        
        return self.classify_risk(processed_data)
    
    def save_results(self, results: List[Dict], filename: str = 'output.json'):
        """
        Save classification results to a JSON file.
        
        Args:
            results (List[Dict]): Classified results
            filename (str): Output filename
        """
        try:
            with open(filename, "w", encoding='utf-8') as f:
                json.dump(results, f, indent=4, ensure_ascii=False)
            logger.info(f"Results saved to {filename}")
        except IOError as e:
            logger.error(f"Error saving results: {e}")

def main():
    inference_engine = GlobalHealthDataInference()
    
    # Example mock local data
    mock_local_data = [
        {
            "source": "Cape Town News",
            "headline": "Malaria cases rising in northern regions",
            "content": "Recent reports indicate a significant rise in malaria cases in northern Cape Town.",
            "date": "2024-12-01",
        }
    ]
    
    try:
        results = inference_engine.process_health_data(mock_local_data)
        for item in results:
            print(f"Content: {item['content']}\nRisk: {item['risk']}\n")
        
        inference_engine.save_results(results)
    
    except Exception as e:
        logger.error(f"Processing failed: {e}")

if __name__ == "__main__":
    main()







import requests
from bs4 import BeautifulSoup
import json
import re
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from transformers import pipeline

# Mock Data for Local Health Sources
mock_local_data = [
    {
        "source": "Cape Town News",
        "headline": "Malaria cases rising in northern regions",
        "content": "Recent reports indicate a significant rise in malaria cases in northern Cape Town.",
        "date": "2024-12-01",
    },
    {
        "source": "Durban Health Advisory",
        "headline": "Cholera outbreak in coastal towns",
        "content": "Local hospitals have reported an outbreak of cholera in Durban's coastal areas.",
        "date": "2024-11-15",
    },
]

# Function to scrape data from CDC/WHO
def scrape_global_data(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        # Example: Extract headlines and summaries
        articles = soup.find_all("article")  # Modify based on structure
        data = [{"title": art.find("h2").text, "summary": art.find("p").text} for art in articles]
        return data
    except Exception as e:
        print(f"Error scraping {url}: {e}")
        return []

# Data Preprocessing Function
def preprocess_text(text):
    text = re.sub(r"\s+", " ", text)  # Remove extra whitespace
    text = re.sub(r"[^\w\s]", "", text)  # Remove punctuation
    return text.lower()

# Risk Classification using Transformers (Mock Example)
def classify_risk(content_list):
    classifier = pipeline("text-classification", model="distilbert-base-uncased")
    categories = []
    for content in content_list:
        result = classifier(content)
        if "malaria" in content.lower():
            categories.append({"content": content, "risk": "High"})
        elif "cholera" in content.lower():
            categories.append({"content": content, "risk": "Moderate"})
        else:
            categories.append({"content": content, "risk": "Low"})
    return categories

# Mock Data Integration
def integrate_data(global_data, local_data):
    combined_data = global_data + local_data
    return pd.DataFrame(combined_data)

# Main Algorithm
def main():
    # Scrape CDC/WHO data
    cdc_data = scrape_global_data("https://www.cdc.gov/travel")
    who_data = scrape_global_data("https://www.who.int/emergencies")
    
    # Preprocess and combine data
    mock_local_data_cleaned = [
        {"source": d["source"], "headline": preprocess_text(d["headline"]), "content": preprocess_text(d["content"]), "date": d["date"]}
        for d in mock_local_data
    ]
    
    # Combine and classify
    combined_data = integrate_data(cdc_data, mock_local_data_cleaned)
    risk_classified = classify_risk(combined_data["content"].tolist())
    
    # Output results
    for item in risk_classified:
        print(f"Content: {item['content']}\nRisk: {item['risk']}\n")
    
    # Save to JSON
    with open("output.json", "w") as f:
        json.dump(risk_classified, f, indent=4)
    print("Data saved to output.json")

if __name__ == "__main__":
    main()
